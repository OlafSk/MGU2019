{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Metody głębokiego uczenia, projekt nr 1\"\n",
    "subtitle: \"Własna implementacja algorytmu wstecznej propagacji błędu w perceptronie wielowarstwowym (MLP)\"\n",
    "author:\n",
    "- Tymoteusz Makowski\n",
    "- Olaf Skrabacz\n",
    "date: \"19 marca 2019\"\n",
    "documentclass: scrartcl\n",
    "---\n",
    "\\thispagestyle{empty}\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opis zadania\n",
    "\n",
    "Celem projektu była implementacja perceptronu wielowarstwowego (ang. *multilayer perceptron*) z szeregiem wymaganych funkcjonalności takich jak:\n",
    "\n",
    "* wybór liczby warstw oraz liczby neuronów ukrytych w każdej warstwie,\n",
    "* wybór funkcji aktywacji,\n",
    "* możliwość ustawienia:\n",
    "    * liczby iteracji,\n",
    "    * wartości współczynnika nauki (ang. *learning rate*),\n",
    "    * wartości współczynnika bezwładności,\n",
    "* możliwość zastosowania sieci zarówno do klasyfikacji, jak i do regresji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementacja\n",
    "\n",
    "Do wykonania zadania projektowego wybraliśmy język programowania Python3 i skorzystaliśmy z jego możliwości obiektowych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcje aktywacji\n",
    "\n",
    "Zaimplementowaliśmy wiele funkcji aktywacji, które można wybierać dla poszczególnych warstw. Oprócz funkcji liniowej zaimplementowaliśmy:\n",
    "\n",
    "### *ReLU* (Rectified Linear Unit)\n",
    "\\begin{equation}\n",
    "\\mathrm{relu}(x) = \\begin{cases}\n",
    "x, & x>0\\\\\n",
    "0, & x\\leq0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "### Funkcja sigmoidalna\n",
    "\\begin{equation}\n",
    "\\mathrm{sigmoid}(x) = \\frac{\\mathrm{e}^x}{1 + \\mathrm{e}^x}\n",
    "\\end{equation}\n",
    "\n",
    "### Funkcja *tanh*\n",
    "\\begin{equation}\n",
    "\\tanh(x) = \\frac{2}{1 + \\mathrm{e}^{-2x}} - 1\n",
    "\\end{equation}\n",
    "\n",
    "### Funkcja wektorowa *softmax*\n",
    "\\begin{equation}\n",
    "\\mathrm{softmax}\\big( (x_i)_{i=1}^n \\big) = \\bigg( \\frac{\\mathrm{e}^{x_i}}{\\sum_{j=1}^n \\mathrm{e}^{x_j}} \\bigg)_{i=1}^n\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasa warstwy *(Layer)*\n",
    "\n",
    "Podczas tworzenia każdej z warstw podajemy następujące parametry:\n",
    "\n",
    "* liczba neuronów, którą ma zawierać ta warstwa,\n",
    "* liczba neuronów poprzedniej warstwy albo, w przypadku pierwszej warstwy, wymiar danych wejściowych,\n",
    "* jedna z funkcji aktywacji wymienionych powyżej.\n",
    " \n",
    "Przykład tworzenia warstwy o 3 neuronach, gdzie dane wejściowe mają dwa wymiary (albo poprzednia warstwa ma dwa neurony), a funkcją aktywacji jest funkcja sigmoidalna:\n",
    "\n",
    ">     Layer(3, 2, \"sigmoid\")\n",
    "\n",
    "Klasa *Layer* nie zawiera metod, które są wykorzystywane z perspektywy użytkownika."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasa sieci *(NeuralNetwork)*\n",
    "\n",
    "Konstruktor klasy *NeuralNetwork* przyjmuje następujące parametry:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
