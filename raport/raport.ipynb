{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Metody głębokiego uczenia, projekt nr 1\"\n",
    "subtitle: \"Własna implementacja algorytmu wstecznej propagacji błędu w perceptronie wielowarstwowym (MLP)\"\n",
    "author:\n",
    "- Tymoteusz Makowski\n",
    "- Olaf Skrabacz\n",
    "date: \"19 marca 2019\"\n",
    "documentclass: scrartcl\n",
    "geometry: margin=2.5cm\n",
    "---\n",
    "\\thispagestyle{empty}\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opis zadania\n",
    "\n",
    "Celem projektu była implementacja perceptronu wielowarstwowego (ang. *multilayer perceptron*) z szeregiem wymaganych funkcjonalności takich jak:\n",
    "\n",
    "* wybór liczby warstw oraz liczby neuronów ukrytych w każdej warstwie,\n",
    "* wybór funkcji aktywacji,\n",
    "* możliwość ustawienia:\n",
    "    * liczby iteracji,\n",
    "    * wartości współczynnika nauki (ang. *learning rate*),\n",
    "    * wartości współczynnika bezwładności,\n",
    "* możliwość zastosowania sieci zarówno do klasyfikacji, jak i do regresji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementacja\n",
    "\n",
    "Do wykonania zadania projektowego wybraliśmy język programowania Python3 i skorzystaliśmy z jego możliwości obiektowych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcje aktywacji\n",
    "\n",
    "Zaimplementowaliśmy wiele funkcji aktywacji, które można wybierać dla poszczególnych warstw. Oprócz funkcji liniowej zaimplementowaliśmy:\n",
    "\n",
    "### *ReLU* (Rectified Linear Unit)\n",
    "\\begin{equation}\n",
    "\\mathrm{relu}(x) = \\begin{cases}\n",
    "x, & x>0\\\\\n",
    "0, & x\\leq0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "### Funkcja sigmoidalna\n",
    "\\begin{equation}\n",
    "\\mathrm{sigmoid}(x) = \\frac{\\mathrm{e}^x}{1 + \\mathrm{e}^x}\n",
    "\\end{equation}\n",
    "\n",
    "### Funkcja *tanh*\n",
    "\\begin{equation}\n",
    "\\tanh(x) = \\frac{2}{1 + \\mathrm{e}^{-2x}} - 1\n",
    "\\end{equation}\n",
    "\n",
    "### Funkcja wektorowa *softmax*\n",
    "\\begin{equation}\n",
    "\\mathrm{softmax}\\big( (x_i)_{i=1}^n \\big) = \\bigg( \\frac{\\mathrm{e}^{x_i}}{\\sum_{j=1}^n \\mathrm{e}^{x_j}} \\bigg)_{i=1}^n\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcje straty\n",
    "\n",
    "W projekcie są do wyboru dwa sposoby obliczania strat. Jest to błąd średniokwadratowy (ang. *mean squared error*) oraz entropia krzyżowa (ang. *cross entropy*). Pierwsza metoda jest wykorzystywana do regresji, zaś druga do klasyfikacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasa warstwy `Layer`\n",
    "\n",
    "Podczas tworzenia każdej z warstw podajemy następujące parametry:\n",
    "\n",
    "* liczba neuronów, którą ma zawierać ta warstwa,\n",
    "* liczba neuronów poprzedniej warstwy albo, w przypadku pierwszej warstwy, wymiar danych wejściowych,\n",
    "* jedna z funkcji aktywacji wymienionych powyżej.\n",
    " \n",
    "Przykład tworzenia warstwy o 3 neuronach, gdzie dane wejściowe mają dwa wymiary (albo poprzednia warstwa ma dwa neurony), a funkcją aktywacji jest funkcja sigmoidalna:\n",
    "\n",
    ">     Layer(3, 2, \"sigmoid\")\n",
    "\n",
    "Klasa `Layer` nie zawiera metod, które są wykorzystywane z perspektywy użytkownika."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasa sieci `NeuralNetwork`\n",
    "\n",
    "Konstruktor klasy `NeuralNetwork` przyjmuje następujące parametry:\n",
    "\n",
    "* rodzaj funkcji błędu,\n",
    "* wartość współczynnika bezwładności.\n",
    "\n",
    "Klasa ta zawiera dwie główne metody -- `add` oraz `train`, które służą do, odpowiednio, dodawania warstwy do sieci i ćwiczenia sieci. Funkcja `train`, oprócz nauki, zwraca na koniec wartości funkcji straty na zbiorze treningowym w kolejnych etapach procesu uczenia.\n",
    "\n",
    "Przykład budowy i uczenia, sieci dwuwarstwowej o liczbie neuronów, kolejno, 1 i 2, do klasyfikacji zbioru na płaszczyźnie.\n",
    "\n",
    ">     nn = NeuralNetwork(\"cross_entropy\", momentum=0)\n",
    ">     nn.add(Layer(1, 2, \"relu\"))\n",
    ">     nn.add(Layer(2, 2, \"softmax\"))\n",
    ">     nn.train(X=train_set_X, Y=train_set_y, epochs=30, learning_rate=0.01)\n",
    "\n",
    "Gdzie `train_set_X` i `train_set_y` to dane treningowe, `epochs` to liczba iteracji uczenia, a `learning_rate` to współczynnik nauki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza działania sieci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresja\n",
    "### Testy\n",
    "...\n",
    "\n",
    "### Wnioski\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikacja\n",
    "### Testy\n",
    "...\n",
    "\n",
    "### Wnioski\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podsumowanie\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
